# FunAsréƒ¨ç½²å¼€å‘

githubä»“åº“å¼€æºåœ°å€

```
https://github.com/modelscope/FunASR/tree/main
```

## å¿«é€Ÿä¸Šæ‰‹

### dockerå®‰è£…

å¦‚æœæ‚¨å·²å®‰è£…dockerï¼Œå¿½ç•¥æœ¬æ­¥éª¤ï¼! é€šè¿‡ä¸‹è¿°å‘½ä»¤åœ¨æœåŠ¡å™¨ä¸Šå®‰è£…dockerï¼š

```
curl -O https://isv-data.oss-cn-hangzhou.aliyuncs.com/ics/MaaS/ASR/shell/install_docker.shï¼›
sudo bash install_docker.sh
```

dockerå®‰è£…å¤±è´¥è¯·å‚è€ƒ [Docker Installation](https://alibaba-damo-academy.github.io/FunASR/en/installation/docker.html)

### é•œåƒå¯åŠ¨

é€šè¿‡ä¸‹è¿°å‘½ä»¤æ‹‰å–å¹¶å¯åŠ¨FunASRè½¯ä»¶åŒ…çš„dockeré•œåƒï¼š

```sh
sudo docker pull \
  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.6
  
mkdir -p ./funasr-runtime-resources/models

sudo docker run -p 10095:10095 -it --privileged=true \
  --name funasr_service\
  -v $PWD/funasr-runtime-resources/models:/workspace/models \
  registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-cpu-0.4.6
```

ä»¥å®¹å™¨åç§°è¿›å…¥æ­¤å®¹å™¨ï¼ˆåç»­ç»´æŠ¤æ–¹ä¾¿ï¼‰ï¼š

```
docker exec -it funasr_service bash
```



### æœåŠ¡ç«¯å¯åŠ¨

dockerå¯åŠ¨ä¹‹åï¼Œè¿›å…¥åˆ°dockeré‡Œè¾¹å¯åŠ¨ funasr-wss-serveræœåŠ¡ç¨‹åºï¼š

```sh
cd FunASR/runtime
#åå°è¿è¡Œ
nohup bash run_server_2pass.sh \
  --download-model-dir /workspace/models \
  --certfile 0 \
  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \
  --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx  \
  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx \
  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \
  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \
  --itn-dir thuduj12/fst_itn_zh \
  --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &
  

#éåå°è¿è¡Œ
bash run_server_2pass.sh \
  --download-model-dir /workspace/models \
  --certfile 0 \
  --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx \
  --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx  \
  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx \
  --punc-dir damo/punc_ct-transformer_cn-en-common-vocab471067-large-onnx \
  --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst \
  --itn-dir thuduj12/fst_itn_zh \
  --hotword /workspace/models/hotwords.txt

## å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚
# For commercial use, please contact the author for authorization. For non-commercial use, please indicate the source.
# åè®®(License)ï¼šç½²å-éå•†ä¸šæ€§ä½¿ç”¨-ç›¸åŒæ–¹å¼å…±äº« 4.0 å›½é™… (CC BY-NC-SA 4.0)
# ä½œè€…(Author)ï¼šlukeewin
# é“¾æ¥(URL)ï¼šhttps://blog.lukeewin.top/archives/install-funasr-with-docker#toc-head-6
# æ¥æº(Source)ï¼šlukeewinçš„åšå®¢

# åœ¨çº¿æ¨¡å‹
#  --online-model-dir /workspace/models/damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx \
# å¦‚æœæ‚¨æƒ³å…³é—­sslï¼Œå¢åŠ å‚æ•°ï¼š--certfile 0
# å¦‚æœæ‚¨æƒ³ä½¿ç”¨SenseVoiceSmallæ¨¡å‹ã€æ—¶é—´æˆ³ã€nnçƒ­è¯æ¨¡å‹è¿›è¡Œéƒ¨ç½²ï¼Œè¯·è®¾ç½®--model-dirä¸ºå¯¹åº”æ¨¡å‹ï¼š
#   iic/SenseVoiceSmall-onnx
#   damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnxï¼ˆæ—¶é—´æˆ³ï¼‰
#   damo/speech_paraformer-large-contextual_asr_nat-zh-cn-16k-common-vocab8404-onnxï¼ˆnnçƒ­è¯ï¼‰
# å¦‚æœæ‚¨æƒ³åœ¨æœåŠ¡ç«¯åŠ è½½çƒ­è¯ï¼Œè¯·åœ¨å®¿ä¸»æœºæ–‡ä»¶./funasr-runtime-resources/models/hotwords.txté…ç½®çƒ­è¯ï¼ˆdockeræ˜ å°„åœ°å€ä¸º/workspace/models/hotwords.txtï¼‰:
#   æ¯è¡Œä¸€ä¸ªçƒ­è¯ï¼Œæ ¼å¼(çƒ­è¯ æƒé‡)ï¼šé˜¿é‡Œå·´å·´ 20ï¼ˆæ³¨ï¼šçƒ­è¯ç†è®ºä¸Šæ— é™åˆ¶ï¼Œä½†ä¸ºäº†å…¼é¡¾æ€§èƒ½å’Œæ•ˆæœï¼Œå»ºè®®çƒ­è¯é•¿åº¦ä¸è¶…è¿‡10ï¼Œä¸ªæ•°ä¸è¶…è¿‡1kï¼Œæƒé‡1~100ï¼‰
# SenseVoiceSmall-onnxè¯†åˆ«ç»“æœä¸­â€œ<|zh|><|NEUTRAL|><|Speech|> â€åˆ†åˆ«ä¸ºå¯¹åº”çš„è¯­ç§ã€æƒ…æ„Ÿã€äº‹ä»¶ä¿¡æ¯


cd /workspace/FunASR/runtime/websocket/build/bin
#åå°è¿è¡Œåœ¨çº¿asræ¨¡å‹
nohup ./funasr-wss-server-2pass 
	--vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx    
    --model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-onnx   
    --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  
    --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx > online_funasr.log 2>&1 &
   
cd /workspace/FunASR/runtime/http/bin





```





### æ­å»ºAPIæœåŠ¡

è¦å®ç°ä½ éœ€è¦çš„åŠŸèƒ½ï¼Œæ•´ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

### ä¸€ã€å°†ç«‹ä½“å£°éŸ³é¢‘ï¼ˆåŒé€šé“ï¼‰å·¦å³å£°é“åˆ†ç¦»

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®ç°åŒé€šé“éŸ³é¢‘åˆ°ä¸¤ä¸ªå•é€šé“éŸ³é¢‘çš„æ‹†åˆ†ã€‚è¯·é€šè¿‡ä¸Šä¼ æ–¹å¼æä¾›ä½ çš„éŸ³é¢‘æ–‡ä»¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Pythonè¿›è¡Œæ‹†åˆ†ã€‚

### äºŒã€ä½¿ç”¨FunASRæ¨¡å‹è¯†åˆ«æ‹†åˆ†åçš„å•é€šé“éŸ³é¢‘

æ‹†åˆ†æˆä¸ºå•é€šé“åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨FunASRæ¨¡å‹åšè¯­éŸ³å†…å®¹è¯†åˆ«ã€‚æ¨èä½¿ç”¨è¾ƒå‡†ç¡®ä¸”é€šç”¨çš„ä¸­æ–‡è¯†åˆ«æ¨¡å‹ï¼Œå¦‚ï¼š

- `damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch` (ä¸­æ–‡é€šç”¨è¯†åˆ«æ¨¡å‹)
- `damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx` (è‡ªå¸¦VADå’Œæ ‡ç‚¹æ¢å¤åŠŸèƒ½çš„æ¨¡å‹)

å¦‚æœä½ çš„éœ€æ±‚ä¸­æœ‰é•¿éŸ³é¢‘å¤„ç†å’Œç«¯ç‚¹æ£€æµ‹ï¼Œå¯ä»¥å†æ­é…`vad-dir`æ¨¡å‹ï¼Œæ¯”å¦‚ï¼š

- `damo/speech_fsmn_vad_zh-cn-16k-common-onnx`

### ä¸‰ã€æ„å»ºAPIæœåŠ¡ï¼Œå®ç°ä¸Šä¼ å¹¶è¿”å›è¯†åˆ«ç»“æœ

æ¥ä¸‹æ¥æˆ‘ä»¬åˆ©ç”¨Pythonçš„FastAPIæ­å»ºAPIï¼Œé€šè¿‡HTTPè¯·æ±‚ä¸Šä¼ æ–‡ä»¶è¿›è¡Œè¯†åˆ«åè¿”å›è¯†åˆ«ç»“æœï¼Œæ›´ä¾¿äºåç»­ç›´æ¥è°ƒç”¨ã€‚

```python
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import JSONResponse
import os
import uuid
from pydub import AudioSegment
from funasr import AutoModel
import shutil

app = FastAPI()

# åŠ è½½ä½ æƒ³ç”¨çš„FunASRæ¨¡å‹ï¼ˆæ¨èæœ€ä½³æ•ˆæœï¼‰
model_dir = "damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx"
asr_model = AutoModel(model=model_dir)

@app.post("/recognize_stereo/")
async def recognize_stereo_audio(file: UploadFile = File(...)):
    # ç”Ÿæˆå”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶å
    temp_filename = f"temp_{uuid.uuid4().hex}.wav"
    
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    with open(temp_filename, 'wb') as buffer:
        shutil.copyfileobj(file.file, buffer)

    try:
        # ä½¿ç”¨pydubåŠ è½½wavéŸ³é¢‘å¹¶æ‹†åˆ†ç«‹ä½“å£°ä¸ºå·¦å³å£°é“
        audio = AudioSegment.from_wav(temp_filename)
        left_channel = audio.split_to_mono()[0]  # å·¦å£°é“
        right_channel = audio.split_to_mono()[1] # å³å£°é“

        # åˆ†åˆ«ä¿å­˜å·¦å³å£°é“æ–‡ä»¶
        left_file_path = f"left_channel_{uuid.uuid4().hex}.wav"
        right_file_path = f"right_channel_{uuid.uuid4().hex}.wav"
        left_channel.export(left_file_path, format="wav")
        right_channel.export(right_file_path, format="wav")

        # ä½¿ç”¨FunASRæ¨¡å‹è¯†åˆ«å·¦å£°é“å’Œå³å£°é“éŸ³é¢‘æ–‡ä»¶
        left_result = asr_model.generate(input=left_file_path)
        right_result = asr_model.generate(input=right_file_path)

        # å®Œæˆè¯†åˆ«ååˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
        os.remove(temp_filename)
        os.remove(left_file_path)
        os.remove(right_file_path)

        # è¿”å›è¯†åˆ«ç»“æœ
        result = {
            "left_channel_text": left_result[0].get('text', ''),
            "right_channel_text": right_result[0].get('text', '')
        }

        return JSONResponse(result)

    except Exception as e:
        # å¦‚æœå‡ºé”™ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
        return JSONResponse({"error": str(e)})

@app.get("/")
def read_root():
    return {"status": "FunASR Stereo API Server is running"}
```

## æ­¥éª¤ä¸‰ï¼šå¯åŠ¨æœåŠ¡å¹¶å¯¹å¤–æš´éœ²æ¥å£

å‡è®¾ä½ å°†ä¸Šè¿°ä»£ç ä¿å­˜ä¸ºï¼š`funasr_api.py`

å¯åŠ¨FastAPIæœåŠ¡ï¼š

```
uvicorn funasr_api:app --host 0.0.0.0 --port 8000
```

æ­¤æ—¶æœåŠ¡å¯¹å¤–æ¥å£åœ°å€ä¸ºï¼š

```
http://æœåŠ¡å™¨IPåœ°å€:8000/recognize_stereo/
```

å¯ä»¥é€šè¿‡HTTP POSTè¯·æ±‚ä¸Šä¼ æ–‡ä»¶è¿›è¡Œè¯­éŸ³è¯†åˆ«ï¼š

```
curl -X POST "http://localhost:8000/recognize_stereo/" -F "file=@your-stereo-audio-file.wav"
```

### ç¼–å†™Dockerfileï¼ˆæ„å»ºä½ çš„é•œåƒï¼‰

é¦–å…ˆï¼Œåœ¨æœ¬åœ°åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œä¾‹å¦‚`funasr-api-service`ï¼Œç„¶ååœ¨é‡Œé¢åˆ›å»ºæ–‡ä»¶ `Dockerfile`ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```dockerfile
FROM python:3.8-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ– (ffmpeg ä¸ºå¤„ç†ç«‹ä½“å£°éŸ³é¢‘çš„ä¾èµ–)
RUN apt-get update && apt-get install -y ffmpeg curl git \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…å¿…è¦çš„ Python åº“
RUN pip install --upgrade pip
RUN pip install fastapi uvicorn pydub funasr-runtime-sdk-cpu

# æ‹·è´ä½ çš„APIä»£ç åˆ°å®¹å™¨
COPY funasr_api.py /app/funasr_api.py

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨FastAPIæœåŠ¡ç¨‹åº
CMD ["uvicorn", "funasr_api:app", "--host", "0.0.0.0", "--port", "8000"]
```

### äºŒã€æ„å»ºé•œåƒ

åœ¨`funasr-api-service`ç›®å½•ä¸‹æ‰§è¡ŒDockerå‘½ä»¤æ„å»ºé•œåƒï¼š

```
docker build -t funasr-api-image .
```

æ„å»ºå®Œæˆåï¼Œä½ ä¼šå¾—åˆ°åä¸º`funasr-api-image`çš„Dockeré•œåƒã€‚

------

### ä¸‰ã€å¯åŠ¨Dockerå®¹å™¨å¹¶å¼€æ”¾ç«¯å£

å¯åŠ¨ä½ çš„Dockerå®¹å™¨ï¼ŒæŠŠå®¹å™¨å†…éƒ¨ç«¯å£8000æ˜ å°„åˆ°ä½ å®¿ä¸»æœºå™¨çš„ç«¯å£8000ï¼ˆä¹Ÿå¯è‡ªå·±è®¾å®šå…¶ä»–ç«¯å£ï¼‰ï¼š

```
docker run -d --name funasr-api-container -p 8000:8000 funasr-api-image
```

æ­¤æ—¶ï¼Œä½ çš„Dockerä¸­çš„æœåŠ¡å¯¹å¤–å·²ç»æš´éœ²ï¼Œç°åœ¨å¯é€šè¿‡å®¿ä¸»æœºIPï¼ˆæœåŠ¡å™¨IPåœ°å€ï¼‰è®¿é—®ï¼š

```
http://æœåŠ¡å™¨IP:8000/recognize_stereo/
```

### å››ã€æµ‹è¯•æœåŠ¡

ä¾‹å¦‚ï¼Œä½ å¯ä»¥æµ‹è¯•APIæ˜¯å¦æˆåŠŸå¯¹å¤–æš´éœ²ï¼š

```
curl -X POST "http://æœåŠ¡å™¨IP:8000/recognize_stereo/" -F "file=@your-stereo-audio-file.wav"
```





### å…·ä½“è§£å†³æ–¹æ³•æ­¥éª¤å¦‚ä¸‹ï¼š

## ğŸš© ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç£ç›˜ç©ºé—´æƒ…å†µ

åœ¨Linuxç¯å¢ƒï¼Œå…ˆæ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥ä½ ç£ç›˜å ç”¨æƒ…å†µï¼š

bash

æŠ˜å ä¿å­˜å¤åˆ¶

1

df -h

å®ƒä¼šæ˜¾ç¤ºç£ç›˜å ç”¨çŠ¶å†µï¼š

bash

æŠ˜å ä¿å­˜å¤åˆ¶

1

2

Filesystem      Size  Used Avail Use% Mounted on

/dev/sda1        50G   49G    0G 100% /

ä¸Šè¿°ç¤ºä¾‹è¡¨ç¤ºæ ¹ç›®å½•ç£ç›˜(/) å·²æ»¡(100%)ï¼Œä½ å°±è¦é‡Šæ”¾ç£ç›˜ç©ºé—´ã€‚

------

## ğŸš¨ ç¬¬äºŒæ­¥ï¼šæ¸…ç†ç£ç›˜ç©ºé—´ï¼ˆå¿…é¡»æ‰§è¡Œï¼‰

æ¨èå¿«é€Ÿå®‰å…¨æœ‰æ•ˆçš„ä¸€äº›æ¸…ç†åŠ¨ä½œï¼š

â‘  æ¸…ç†aptç¼“å­˜æ–‡ä»¶ï¼ˆå¦‚æœæ˜¯Ubunutuæˆ–Debianï¼‰ï¼š

bash

æŠ˜å ä¿å­˜å¤åˆ¶

```
sudo apt-get clean
sudo apt-get autoclean
```

â‘¡ æ¸…ç† pip ç¼“å­˜ï¼Œé‡Šæ”¾è¾ƒå¤šç£ç›˜ç©ºé—´ï¼š

```
pip cache purge
```

â‘¢ æ¸…ç†ä¸´æ—¶åƒåœ¾æ–‡ä»¶ï¼š

```
sudo rm -rf /tmp/*
sudo rm -rf /var/tmp/*
```

â‘£ æ¸…ç†æ— ç”¨çš„dockeré•œåƒæˆ–å®¹å™¨ï¼ˆè‹¥æœ‰dockerç¯å¢ƒï¼‰ï¼š

```
docker system prune -a -f
```

â‘¤ åˆ é™¤ä¸å†ä½¿ç”¨çš„ç³»ç»Ÿæ—¥å¿—æˆ–æ–‡ä»¶ï¼ˆå¦‚è€æ—§çš„logsæˆ–æ–‡ä»¶ï¼‰ï¼š

```
sudo journalctl --vacuum-size=500M  # æ¸…ç†æ—¥å¿—åˆ°500M

sudo rm -rf /var/log/*.gz

```

```
sudo apt-get clean && sudo apt-get autoclean
pip cache purge
sudo journalctl --vacuum-size=500M
sudo rm -rf /tmp/* /var/tmp/* /var/log/*.gz
docker system prune -a -f
df -h
```



/root/autodl-tmp/FunASR
/autodl-tmp/FunASR/python/wav_file

### **1. ç¡®ä¿å·²å®‰è£… GPU ç‰ˆ PyTorch**

FunASR ä¾èµ– PyTorchï¼ˆGPU ç‰ˆï¼‰ï¼Œé¦–å…ˆéªŒè¯æ˜¯å¦æ­£ç¡®å®‰è£…ï¼š

```
python -c "import torch; print(torch.__version__, torch.cuda.is_available())"
```

å¦‚æœè¾“å‡º `False`ï¼Œè¯´æ˜ PyTorch æœªå¯ç”¨ GPUï¼Œéœ€é‡æ–°å®‰è£…ï¼š

```
pip install torch==2.1.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cu118  # CUDA 11.8
```

### **2. å®‰è£… FunASRï¼ˆGPU ç‰ˆæœ¬ï¼‰**

#### **æ–¹æ³• 1ï¼šç›´æ¥å®‰è£…ï¼ˆæ¨èï¼‰**

```
pip install "funasr[gpu]" -f https://modelscope.cn/packages/funasr/releases.html
```

è¿™ä¼šè‡ªåŠ¨å®‰è£… GPU ä¾èµ–ï¼ˆå¦‚ `onnxruntime-gpu`ï¼‰ã€‚

# æŸ¥çœ‹æ‰€æœ‰ç¯å¢ƒ
```
conda env list
```



# æ¿€æ´»ä½ çš„ FunASR ç¯å¢ƒï¼ˆä¾‹å¦‚ funasr_envï¼‰
```
conda activate funasr_env
```



# ç¡®è®¤å½“å‰ Python ç¯å¢ƒ
```
which python
pip list | grep funasr
```



ä¸‹è½½åŠ é€Ÿ

```
source /etc/network_turbo
```



miniconda3å®‰è£…

```
https://cloud.tencent.com/developer/article/1769751?policyId=1004
```

openweb-uiæ‰äº†çš„è¯è¦é‡è£…

```
pip install open-webui
```

åå°å¯åŠ¨

```
nohup open-webui serve --port 6006 > openwebui.log 2>&1 &
```

```
pip install "funasr[gpu]" -f https://modelscope.cn/packages/funasr/releases.html --target=/root/autodl-tmp/databak/data/pip
pip install --target=/root/autodl-tmp/databak/data/pip modelscope
pip install --target=/root/autodl-tmp/databak/data/pip torch torchaudio
```



é‡Œé¢å…¶å®å¯ä»¥ä¼ urlçš„

![image-20250509194449879](https://lyx-study-note-image.oss-cn-shenzhen.aliyuncs.com/img/image-20250509194449879.png)



apiåŒ…å®‰è£…

```
pip install uvicorn fastapi sqlalchemy pydantic_settings pymysql 
```





### Funasrå®Œæ•´éƒ¨ç½²ä»£ç 

```python
import logging
import sys
from logging.handlers import TimedRotatingFileHandler
from datetime import datetime
import os
import psutil
import torch
import gc
import numpy as np
from scipy.io import wavfile
import re
import concurrent.futures

# æ—¥å¿—é…ç½®ï¼ŒåŠ¡å¿…æ”¾åœ¨æ‰€æœ‰importä¹‹å‰
log_formatter = logging.Formatter(
    "%(asctime)s %(levelname)s %(filename)s:%(lineno)d %(message)s"
)

# æ§åˆ¶å°æ—¥å¿—
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(log_formatter)

# æŒ‰å¤©åˆ‡åˆ†çš„æ–‡ä»¶æ—¥å¿—ï¼Œä¿ç•™7å¤©
file_handler = TimedRotatingFileHandler(
    "audio_recognition_api.log",
    when="midnight",           # æ¯å¤©0ç‚¹åˆ‡åˆ†
    interval=1,
    backupCount=7,             # ä¿ç•™æœ€è¿‘7å¤©æ—¥å¿—
    encoding="utf-8",
    utc=False
)
file_handler.setFormatter(log_formatter)

logging.basicConfig(
    level=logging.INFO,
    handlers=[console_handler, file_handler]
)

logger = logging.getLogger(__name__)

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import time
import os
import tempfile
from pathlib import Path
from funasr import AutoModel
from pydub import AudioSegment
import httpx
import asyncio
from concurrent.futures import ThreadPoolExecutor
import sqlite3
import threading
import requests
import pymysql
import uuid
import shutil

app = FastAPI()

TEMP_ROOT = Path("/root/autodl-tmp/FunASR/develop/wav_data")
# TEMP_ROOT = Path("D:/wav_data")
TEMP_ROOT.mkdir(parents=True, exist_ok=True)

# æ•°æ®åº“æ–‡ä»¶è·¯å¾„
SQLITE_DB_PATH = "tasks.db"

class RecognitionRequest(BaseModel):
    callRecordId: int
    fileUrl: str
    channelCode: str
    applyNo: str
    modelChannel: str
    callbackUrl: str
    doubaoEndpoint: str

def download_audio_from_url_sync(url, call_record_id, save_path=None):
    """åŒæ­¥ä¸‹è½½éŸ³é¢‘æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œæ–‡ä»¶åå”¯ä¸€"""
    try:
        response = requests.get(url, timeout=10, stream=True)
        response.raise_for_status()
        if save_path is None:
            temp_dir = TEMP_ROOT / f"funasr_temp_{call_record_id}_{uuid.uuid4().hex[:8]}"
            temp_dir.mkdir(exist_ok=True)
            save_path = temp_dir / f"downloaded_audio_{call_record_id}.wav"
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return str(save_path), temp_dir
    except Exception as e:
        return None, None

def split_stereo_audio(input_file, left_output_file, right_output_file, sample_rate=16000):
    audio = AudioSegment.from_wav(input_file)
    left = audio.split_to_mono()[0].set_frame_rate(sample_rate)
    right = audio.split_to_mono()[1].set_frame_rate(sample_rate)
    left.export(left_output_file, format="wav")
    right.export(right_output_file, format="wav")

# def ensure_wav_16k1ch(input_file, output_file):
#     """
#     è‡ªåŠ¨å°†éŸ³é¢‘è½¬æ¢ä¸º16kHzã€å•å£°é“ã€WAVæ ¼å¼
#     """
#     audio = AudioSegment.from_file(input_file)
#     audio = audio.set_frame_rate(16000).set_channels(1)
#     audio.export(output_file, format="wav")
#     return output_file

def recognize_audio(asr_model, audio_file):
    results = asr_model.generate(input=audio_file)
    return results

async def async_recognize_audio(asr_model, audio_file):
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor() as pool:
        result = await loop.run_in_executor(pool, recognize_audio, asr_model, audio_file)
    return result

# def check_audio_valid(audio_path, min_duration_ms=1000):
#     try:
#         audio = AudioSegment.from_wav(audio_path)
#         logger.info(f"éŸ³é¢‘æ£€æŸ¥: {audio_path}, æ—¶é•¿: {len(audio)}ms, é‡‡æ ·ç‡: {audio.frame_rate}, å£°é“: {audio.channels}")
#         if len(audio) < min_duration_ms:
#             raise Exception(f"éŸ³é¢‘è¿‡çŸ­: {len(audio)}ms")
#         if audio.frame_rate != 16000:
#             raise Exception(f"é‡‡æ ·ç‡ä¸æ˜¯16k: {audio.frame_rate}")
#         if audio.channels < 1:
#             raise Exception(f"å£°é“æ•°å¼‚å¸¸: {audio.channels}")
#         return True
#     except Exception as e:
#         logger.error(f"éŸ³é¢‘æœ‰æ•ˆæ€§æ£€æŸ¥å¤±è´¥: {audio_path}, error: {e}")
#         return False

def log_resource_usage():
    process = psutil.Process(os.getpid())
    logger.info(f"å†…å­˜å ç”¨: {process.memory_info().rss / 1024 / 1024:.2f} MB, æ‰“å¼€æ–‡ä»¶æ•°: {process.num_fds()}")

# åœ¨å…¨å±€åªåŠ è½½ä¸€æ¬¡æ¨¡å‹
asr_model = AutoModel(
    model="iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
    vad_model="fsmn-vad",
    punc_model="ct-punc",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
    spk_mode="punc_segment",
    sentence_timestamp=True,
    en_post_proc=True
)

def cleanup_resources():
    gc.collect()
    torch.cuda.empty_cache()

def resource_cleanup_loop(interval=600):
    while True:
        cleanup_resources()
        logger.info("å®šæœŸèµ„æºæ¸…ç†å®Œæˆ")
        time.sleep(interval)

cleanup_thread = threading.Thread(target=resource_cleanup_loop, daemon=True)
cleanup_thread.start()

@app.get("/health")
def health_check():
    mem = psutil.Process().memory_info().rss / 1024 / 1024
    return {"status": "ok", "memory_MB": mem, "threads": threading.active_count()}

@app.post("/recognize-audio")
async def recognize_audio_endpoint(request: RecognitionRequest):
    logger.info(f"æ”¶åˆ°æ–°ä»»åŠ¡è¯·æ±‚: callRecordId={request.callRecordId}, fileUrl={request.fileUrl}")
    try:
        conn = sqlite3.connect(SQLITE_DB_PATH)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                callRecordId INTEGER UNIQUE,
                fileUrl TEXT,
                channelCode TEXT,
                applyNo TEXT,
                modelChannel TEXT,
                callbackUrl TEXT,
                doubaoEndpoint TEXT,
                status TEXT
            )
        ''')
        try:
            cursor.execute('''
                INSERT INTO tasks (callRecordId, fileUrl, channelCode, applyNo, modelChannel, callbackUrl, doubaoEndpoint, status)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                request.callRecordId,
                request.fileUrl,
                request.channelCode,
                request.applyNo,
                request.modelChannel,
                request.callbackUrl,
                request.doubaoEndpoint,
                "pending"
            ))
            conn.commit()
            logger.info(f"ä»»åŠ¡å…¥åº“æˆåŠŸ: callRecordId={request.callRecordId}")
        except sqlite3.IntegrityError:
            logger.warning(f"ä»»åŠ¡å·²å­˜åœ¨: callRecordId={request.callRecordId}")
            conn.close()
            return {"message": f"ä»»åŠ¡å·²å­˜åœ¨ï¼ŒcallRecordId={request.callRecordId}"}
        conn.close()
        return {"message": "ä»»åŠ¡å·²å…¥åº“ï¼Œç¨åå›è°ƒç»“æœ"}
    except Exception as e:
        logger.error(f"ä»»åŠ¡å…¥åº“å¤±è´¥: callRecordId={request.callRecordId}, error={e}")
        raise HTTPException(status_code=500, detail=f"å…¥åº“å¤±è´¥: {e}")

@app.get("/process-pending-tasks")
def process_pending_tasks():
    logger.info("å¼€å§‹å¤„ç†pendingä»»åŠ¡")
    try:
        conn = sqlite3.connect(SQLITE_DB_PATH)
        cursor = conn.cursor()
        cursor.execute('SELECT id FROM tasks WHERE status = "pending"')
        task_ids = [row[0] for row in cursor.fetchall()]
        logger.info(f"æœ¬æ¬¡å¾…å¤„ç†ä»»åŠ¡æ•°: {len(task_ids)}")
        if not task_ids:
            conn.close()
            logger.info("æ— å¾…å¤„ç†ä»»åŠ¡")
            return {"message": "æ— å¾…å¤„ç†ä»»åŠ¡"}
        cursor.executemany('UPDATE tasks SET status = "processing" WHERE id = ?', [(tid,) for tid in task_ids])
        conn.commit()
        cursor.execute(f'SELECT * FROM tasks WHERE id IN ({",".join(["?"]*len(task_ids))})', task_ids)
        tasks = cursor.fetchall()
        processed = 0
        failed = 0

        logger.info("ASRæ¨¡å‹åŠ è½½å®Œæˆ")

        for task in tasks:
            task_id, callRecordId, fileUrl, channelCode, applyNo, modelChannel, callbackUrl, doubaoEndpoint, status = task
            logger.info(f"å¼€å§‹å¤„ç†ä»»åŠ¡: id={task_id}, callRecordId={callRecordId}")
            temp_dir = None
            try:
                logger.info("å‡†å¤‡ä¸‹è½½éŸ³é¢‘")
                input_file, temp_dir = download_audio_from_url_sync(fileUrl, callRecordId)
                logger.info("éŸ³é¢‘ä¸‹è½½å®Œæˆ")
                if input_file is None:
                    raise Exception("æ— æ³•ä¸‹è½½éŸ³é¢‘æ–‡ä»¶")
                logger.info(f"éŸ³é¢‘ä¸‹è½½æˆåŠŸ: {input_file}")

                # ç›´æ¥ç”¨ä¸‹è½½ä¸‹æ¥çš„éŸ³é¢‘æ–‡ä»¶ä½œä¸ºè¾“å…¥
                standard_wav = input_file  # input_file å°±æ˜¯ä¸‹è½½ä¸‹æ¥çš„æ–‡ä»¶è·¯å¾„
                left_audio = temp_dir / f"left_channel_{callRecordId}.wav"
                right_audio = temp_dir / f"right_channel_{callRecordId}.wav"
                logger.info("å‡†å¤‡åˆ†ç¦»éŸ³é¢‘")
                split_stereo_audio(standard_wav, left_audio, right_audio)
                logger.info("éŸ³é¢‘åˆ†ç¦»å®Œæˆ")
                logger.info(f"éŸ³é¢‘åˆ†ç¦»æˆåŠŸ: left={left_audio}, right={right_audio}")

                logger.info(f"å·¦å£°é“æ–‡ä»¶: {left_audio}, å³å£°é“æ–‡ä»¶: {right_audio}")
                # check_audio_valid(left_audio)
                # check_audio_valid(right_audio)

                logger.info("å‡†å¤‡å·¦å£°é“è¯†åˆ«")
                try:
                    left_result = recognize_audio(asr_model, str(left_audio))
                except Exception as e:
                    logger.error(f"å·¦å£°é“è¯†åˆ«å¤±è´¥: {e}")
                    left_result = f"VADè¯†åˆ«å¤±è´¥: {e}"
                    if is_fatal_index_error(e):
                        cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                        conn.commit()
                        failed += 1
                        cleanup_resources()
                        check_and_restart_if_oom(force_restart=True)
                        continue

                try:
                    right_result = recognize_audio(asr_model, str(right_audio))
                except Exception as e:
                    logger.error(f"å³å£°é“è¯†åˆ«å¤±è´¥: {e}")
                    right_result = f"VADè¯†åˆ«å¤±è´¥: {e}"
                    if is_fatal_index_error(e):
                        cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                        conn.commit()
                        failed += 1
                        cleanup_resources()
                        check_and_restart_if_oom(force_restart=True)
                        continue

                logger.info(f"è¯†åˆ«å®Œæˆ: left_result={str(left_result)[:100]}, right_result={str(right_result)[:100]}")

                processing_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                callback_data = {
                    "taskId": str(task_id),
                    "callRecordId": callRecordId,
                    "status": "success",
                    "processingTime": processing_time,
                    "leftChannelResult": str(left_result),
                    "rightChannelResult": str(right_result),
                    "errorMessage": "",
                    "channelCode": channelCode,
                    "applyNo": applyNo,
                    "modelChannel": modelChannel,
                    "doubaoEndpoint": doubaoEndpoint
                }

                logger.info(f"å›è°ƒæ•°æ®: {callback_data}")

                response = requests.post(callbackUrl, json=callback_data)
                response.raise_for_status()
                logger.info(f"å›è°ƒæˆåŠŸ: {callbackUrl}")

                cursor.execute('UPDATE tasks SET status = "completed" WHERE id = ?', (task_id,))
                conn.commit()
                processed += 1

            except Exception as e:
                cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                conn.commit()
                logger.error(f"ä»»åŠ¡å¤„ç†å¤±è´¥: id={task_id}, callRecordId={callRecordId}, error={e}")
                failed += 1
            finally:
                if temp_dir and temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
                log_resource_usage()
                cleanup_resources()

        conn.close()
        logger.info(f"æœ¬æ¬¡å¤„ç†å®Œæˆï¼ŒæˆåŠŸï¼š{processed}ï¼Œå¤±è´¥ï¼š{failed}")
        return {"message": f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸï¼š{processed}ï¼Œå¤±è´¥ï¼š{failed}"}
    except Exception as e:
        logger.error(f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")

def cleanup_resources():
    gc.collect()
    try:
        torch.cuda.empty_cache()
    except Exception:
        pass

def check_and_restart_if_oom(memory_limit_mb=16000, force_restart=False):
    """
    æ£€æŸ¥å†…å­˜å ç”¨ï¼Œæˆ–æ”¶åˆ°è‡´å‘½é”™è¯¯æ—¶å¼ºåˆ¶é‡å¯
    """
    process = psutil.Process(os.getpid())
    mem_mb = process.memory_info().rss / 1024 / 1024
    logger.info(f"å½“å‰å†…å­˜å ç”¨: {mem_mb:.2f} MB")
    if force_restart or mem_mb > memory_limit_mb:
        logger.error(f"è§¦å‘è‡ªåŠ¨é‡å¯ï¼ˆforce={force_restart}ï¼Œå†…å­˜={mem_mb:.2f}MBï¼‰ï¼")
        try:
            # é‡å¯å‰å°†processingä»»åŠ¡é‡ç½®ä¸ºpending
            conn = sqlite3.connect(SQLITE_DB_PATH)
            cursor = conn.cursor()
            cursor.execute('UPDATE tasks SET status = "pending" WHERE status = "processing"')
            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"é‡å¯å‰é‡ç½®processingä»»åŠ¡å¤±è´¥: {e}")
        try:
            import torch
            torch.cuda.empty_cache()
        except Exception:
            pass
        os.execv(sys.executable, [sys.executable] + sys.argv)

def is_fatal_index_error(e):
    """
    åˆ¤æ–­æ˜¯å¦ä¸º index xxx is out of bounds for dimension 0 with size xxx ä¸”ä¸¤æ•°ç›¸ç­‰çš„è‡´å‘½é”™è¯¯
    æˆ– 'list index out of range' è¿™ç±»è‡´å‘½é”™è¯¯
    """
    msg = str(e)
    # æ£€æŸ¥ index out of bounds
    match = re.search(r'index (\d+) is out of bounds for dimension 0 with size (\d+)', msg)
    if match:
        idx, size = int(match.group(1)), int(match.group(2))
        if idx == size:
            return True
    # æ£€æŸ¥ list index out of range
    if "list index out of range" in msg:
        return True
    return False

def recognize_audio_with_timeout(asr_model, audio_file, timeout=60):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(recognize_audio, asr_model, audio_file)
        return future.result(timeout=timeout)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=2000)
```

### Funasrå®Œæ•´ä»£ç ï¼ˆæŠ•å…¥ç”Ÿäº§ä½¿ç”¨ï¼‰

**ä¸Šé¢çš„ä»£ç æ¯”è¾ƒç²—ç³™ï¼Œæ¯æ¬¡éƒ½éœ€è¦åŠ è½½æ¨¡å‹å¹¶ä¸”é€Ÿåº¦åŠæ…¢ï¼Œè¿˜å¾ˆå®¹æ˜“å¯¼è‡´OOMæ‰€ä»¥é€šè¿‡åŠªåŠ›é‡æ–°ä¼˜åŒ–äº†ä¸€ç‰ˆä»£ç ï¼ŒåŸºæœ¬ä¸Šå¯ä»¥3ç§’~5ç§’å¤„ç†ä¸€æ¡éŸ³é¢‘**

```python
import logging
import sys
from logging.handlers import TimedRotatingFileHandler
from datetime import datetime
import os
import psutil
import torch
import gc
import numpy as np
from scipy.io import wavfile
import re
import concurrent.futures

# æ—¥å¿—é…ç½®ï¼ŒåŠ¡å¿…æ”¾åœ¨æ‰€æœ‰importä¹‹å‰
log_formatter = logging.Formatter(
    "%(asctime)s %(levelname)s %(filename)s:%(lineno)d %(message)s"
)

# æ§åˆ¶å°æ—¥å¿—
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(log_formatter)

# æŒ‰å¤©åˆ‡åˆ†çš„æ–‡ä»¶æ—¥å¿—ï¼Œä¿ç•™7å¤©
file_handler = TimedRotatingFileHandler(
    "audio_recognition_api.log",
    when="midnight",           # æ¯å¤©0ç‚¹åˆ‡åˆ†
    interval=1,
    backupCount=7,             # ä¿ç•™æœ€è¿‘7å¤©æ—¥å¿—
    encoding="utf-8",
    utc=False
)
file_handler.setFormatter(log_formatter)

logging.basicConfig(
    level=logging.INFO,
    handlers=[console_handler, file_handler]
)

logger = logging.getLogger(__name__)

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import time
import os
import tempfile
from pathlib import Path
from funasr import AutoModel
from pydub import AudioSegment
import httpx
import asyncio
from concurrent.futures import ThreadPoolExecutor
import sqlite3
import threading
import requests
import pymysql
import uuid
import shutil

app = FastAPI()

TEMP_ROOT = Path("/root/autodl-tmp/FunASR/develop/wav_data")
# TEMP_ROOT = Path("D:/wav_data")
TEMP_ROOT.mkdir(parents=True, exist_ok=True)
# æ•°æ®åº“æ–‡ä»¶è·¯å¾„
SQLITE_DB_PATH = "tasks.db"

class RecognitionRequest(BaseModel):
    callRecordId: int
    fileUrl: str
    channelCode: str
    applyNo: str
    modelChannel: str
    callbackUrl: str
    doubaoEndpoint: str

def download_audio_from_url_sync(url, call_record_id, save_path=None):
    """åŒæ­¥ä¸‹è½½éŸ³é¢‘æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œæ–‡ä»¶åå”¯ä¸€"""
    try:
        response = requests.get(url, timeout=10, stream=True)
        response.raise_for_status()
        if save_path is None:
            temp_dir = TEMP_ROOT / f"funasr_temp_{call_record_id}_{uuid.uuid4().hex[:8]}"
            temp_dir.mkdir(exist_ok=True)
            save_path = temp_dir / f"downloaded_audio_{call_record_id}.wav"
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return str(save_path), temp_dir
    except Exception as e:
        return None, None

def split_stereo_audio(input_file, left_output_file, right_output_file, sample_rate=16000):
    audio = AudioSegment.from_wav(input_file)
    left = audio.split_to_mono()[0].set_frame_rate(sample_rate)
    right = audio.split_to_mono()[1].set_frame_rate(sample_rate)
    left.export(left_output_file, format="wav")
    right.export(right_output_file, format="wav")

# def ensure_wav_16k1ch(input_file, output_file):
#     """
#     è‡ªåŠ¨å°†éŸ³é¢‘è½¬æ¢ä¸º16kHzã€å•å£°é“ã€WAVæ ¼å¼
#     """
#     audio = AudioSegment.from_file(input_file)
#     audio = audio.set_frame_rate(16000).set_channels(1)
#     audio.export(output_file, format="wav")
#     return output_file

def recognize_audio(asr_model, audio_file):
    results = asr_model.generate(input=audio_file)
    return results

async def async_recognize_audio(asr_model, audio_file):
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor() as pool:
        result = await loop.run_in_executor(pool, recognize_audio, asr_model, audio_file)
    return result

# def check_audio_valid(audio_path, min_duration_ms=1000):
#     try:
#         audio = AudioSegment.from_wav(audio_path)
#         logger.info(f"éŸ³é¢‘æ£€æŸ¥: {audio_path}, æ—¶é•¿: {len(audio)}ms, é‡‡æ ·ç‡: {audio.frame_rate}, å£°é“: {audio.channels}")
#         if len(audio) < min_duration_ms:
#             raise Exception(f"éŸ³é¢‘è¿‡çŸ­: {len(audio)}ms")
#         if audio.frame_rate != 16000:
#             raise Exception(f"é‡‡æ ·ç‡ä¸æ˜¯16k: {audio.frame_rate}")
#         if audio.channels < 1:
#             raise Exception(f"å£°é“æ•°å¼‚å¸¸: {audio.channels}")
#         return True
#     except Exception as e:
#         logger.error(f"éŸ³é¢‘æœ‰æ•ˆæ€§æ£€æŸ¥å¤±è´¥: {audio_path}, error: {e}")
#         return False

def log_resource_usage():
    process = psutil.Process(os.getpid())
    logger.info(f"å†…å­˜å ç”¨: {process.memory_info().rss / 1024 / 1024:.2f} MB, æ‰“å¼€æ–‡ä»¶æ•°: {process.num_fds()}")

# åœ¨å…¨å±€åªåŠ è½½ä¸€æ¬¡æ¨¡å‹
asr_model = AutoModel(
    model="iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
    vad_model="fsmn-vad",
    punc_model="ct-punc",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
    spk_mode="punc_segment",
    sentence_timestamp=True,
    en_post_proc=True
)

def cleanup_resources():
    gc.collect()
    torch.cuda.empty_cache()

def resource_cleanup_loop(interval=300):
    while True:
        cleanup_resources()
        logger.info("å®šæœŸèµ„æºæ¸…ç†å®Œæˆ")
        time.sleep(interval)

cleanup_thread = threading.Thread(target=resource_cleanup_loop, daemon=True)
cleanup_thread.start()

@app.get("/health")
def health_check():
    mem = psutil.Process().memory_info().rss / 1024 / 1024
    return {"status": "ok", "memory_MB": mem, "threads": threading.active_count()}

@app.post("/recognize-audio")
async def recognize_audio_endpoint(request: RecognitionRequest):
    logger.info(f"æ”¶åˆ°æ–°ä»»åŠ¡è¯·æ±‚: callRecordId={request.callRecordId}, fileUrl={request.fileUrl}")
    try:
        conn = sqlite3.connect(SQLITE_DB_PATH)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                callRecordId INTEGER UNIQUE,
                fileUrl TEXT,
                channelCode TEXT,
                applyNo TEXT,
                modelChannel TEXT,
                callbackUrl TEXT,
                doubaoEndpoint TEXT,
                status TEXT
            )
        ''')
        try:
            cursor.execute('''
                INSERT INTO tasks (callRecordId, fileUrl, channelCode, applyNo, modelChannel, callbackUrl, doubaoEndpoint, status)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                request.callRecordId,
                request.fileUrl,
                request.channelCode,
                request.applyNo,
                request.modelChannel,
                request.callbackUrl,
                request.doubaoEndpoint,
                "pending"
            ))
            conn.commit()
            logger.info(f"ä»»åŠ¡å…¥åº“æˆåŠŸ: callRecordId={request.callRecordId}")
        except sqlite3.IntegrityError:
            logger.warning(f"ä»»åŠ¡å·²å­˜åœ¨: callRecordId={request.callRecordId}")
            conn.close()
            return {"message": f"ä»»åŠ¡å·²å­˜åœ¨ï¼ŒcallRecordId={request.callRecordId}"}
        conn.close()
        return {"message": "ä»»åŠ¡å·²å…¥åº“ï¼Œç¨åå›è°ƒç»“æœ"}
    except Exception as e:
        logger.error(f"ä»»åŠ¡å…¥åº“å¤±è´¥: callRecordId={request.callRecordId}, error={e}")
        raise HTTPException(status_code=500, detail=f"å…¥åº“å¤±è´¥: {e}")

@app.get("/process-pending-tasks")
def process_pending_tasks():
    logger.info("å¼€å§‹å¤„ç†pendingä»»åŠ¡")
    try:
        conn = sqlite3.connect(SQLITE_DB_PATH)
        cursor = conn.cursor()
        cursor.execute('SELECT id FROM tasks WHERE status = "pending"')
        task_ids = [row[0] for row in cursor.fetchall()]
        logger.info(f"æœ¬æ¬¡å¾…å¤„ç†ä»»åŠ¡æ•°: {len(task_ids)}")
        if not task_ids:
            conn.close()
            logger.info("æ— å¾…å¤„ç†ä»»åŠ¡")
            return {"message": "æ— å¾…å¤„ç†ä»»åŠ¡"}
        cursor.executemany('UPDATE tasks SET status = "processing" WHERE id = ?', [(tid,) for tid in task_ids])
        conn.commit()
        cursor.execute(f'SELECT * FROM tasks WHERE id IN ({",".join(["?"]*len(task_ids))})', task_ids)
        tasks = cursor.fetchall()
        processed = 0
        failed = 0

        logger.info("ASRæ¨¡å‹åŠ è½½å®Œæˆ")

        for task in tasks:
            task_id, callRecordId, fileUrl, channelCode, applyNo, modelChannel, callbackUrl, doubaoEndpoint, status = task
            logger.info(f"å¼€å§‹å¤„ç†ä»»åŠ¡: id={task_id}, callRecordId={callRecordId}")
            temp_dir = None
            try:
                logger.info("å‡†å¤‡ä¸‹è½½éŸ³é¢‘")
                input_file, temp_dir = download_audio_from_url_sync(fileUrl, callRecordId)
                logger.info("éŸ³é¢‘ä¸‹è½½å®Œæˆ")
                if input_file is None:
                    raise Exception("æ— æ³•ä¸‹è½½éŸ³é¢‘æ–‡ä»¶")
                logger.info(f"éŸ³é¢‘ä¸‹è½½æˆåŠŸ: {input_file}")

                # ç›´æ¥ç”¨ä¸‹è½½ä¸‹æ¥çš„éŸ³é¢‘æ–‡ä»¶ä½œä¸ºè¾“å…¥
                standard_wav = input_file  # input_file å°±æ˜¯ä¸‹è½½ä¸‹æ¥çš„æ–‡ä»¶è·¯å¾„
                left_audio = temp_dir / f"left_channel_{callRecordId}.wav"
                right_audio = temp_dir / f"right_channel_{callRecordId}.wav"
                logger.info("å‡†å¤‡åˆ†ç¦»éŸ³é¢‘")
                split_stereo_audio(standard_wav, left_audio, right_audio)
                logger.info("éŸ³é¢‘åˆ†ç¦»å®Œæˆ")
                logger.info(f"éŸ³é¢‘åˆ†ç¦»æˆåŠŸ: left={left_audio}, right={right_audio}")

                logger.info(f"å·¦å£°é“æ–‡ä»¶: {left_audio}, å³å£°é“æ–‡ä»¶: {right_audio}")
                # check_audio_valid(left_audio)
                # check_audio_valid(right_audio)

                logger.info("å‡†å¤‡å·¦å£°é“è¯†åˆ«")
                try:
                    left_result = recognize_audio(asr_model, str(left_audio))
                except Exception as e:
                    logger.error(f"å·¦å£°é“è¯†åˆ«å¤±è´¥: {e}")
                    left_result = f"VADè¯†åˆ«å¤±è´¥: {e}"
                    if is_fatal_index_error(e):
                        cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                        conn.commit()
                        failed += 1
                        cleanup_resources()
                        check_and_restart_if_oom(force_restart=True)
                        continue

                try:
                    right_result = recognize_audio(asr_model, str(right_audio))
                except Exception as e:
                    logger.error(f"å³å£°é“è¯†åˆ«å¤±è´¥: {e}")
                    right_result = f"VADè¯†åˆ«å¤±è´¥: {e}"
                    if is_fatal_index_error(e):
                        cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                        conn.commit()
                        failed += 1
                        cleanup_resources()
                        check_and_restart_if_oom(force_restart=True)
                        continue

                logger.info(f"è¯†åˆ«å®Œæˆ: left_result={str(left_result)[:100]}, right_result={str(right_result)[:100]}")
                processing_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                callback_data = {
                    "taskId": str(task_id),
                    "callRecordId": callRecordId,
                    "status": "success",
                    "processingTime": processing_time,
                    "leftChannelResult": str(left_result),
                    "rightChannelResult": str(right_result),
                    "errorMessage": "",
                    "channelCode": channelCode,
                    "applyNo": applyNo,
                    "modelChannel": modelChannel,
                    "doubaoEndpoint": doubaoEndpoint
                }

                logger.info(f"å›è°ƒæ•°æ®: {callback_data}")

                response = requests.post(callbackUrl, json=callback_data)
                response.raise_for_status()
                logger.info(f"å›è°ƒæˆåŠŸ: {callbackUrl}")

                cursor.execute('UPDATE tasks SET status = "completed" WHERE id = ?', (task_id,))
                conn.commit()
                processed += 1

            except Exception as e:
                cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                conn.commit()
                logger.error(f"ä»»åŠ¡å¤„ç†å¤±è´¥: id={task_id}, callRecordId={callRecordId}, error={e}")
                failed += 1
            finally:
                if temp_dir and temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
                log_resource_usage()
                cleanup_resources()

        conn.close()
        logger.info(f"æœ¬æ¬¡å¤„ç†å®Œæˆï¼ŒæˆåŠŸï¼š{processed}ï¼Œå¤±è´¥ï¼š{failed}")
        return {"message": f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸï¼š{processed}ï¼Œå¤±è´¥ï¼š{failed}"}
    except Exception as e:
        logger.error(f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")

def cleanup_resources():
    gc.collect()
    try:
        torch.cuda.empty_cache()
    except Exception:
        pass

def check_and_restart_if_oom(memory_limit_mb=16000, force_restart=False):
    """
    æ£€æŸ¥å†…å­˜å ç”¨ï¼Œæˆ–æ”¶åˆ°è‡´å‘½é”™è¯¯æ—¶å¼ºåˆ¶é‡å¯
    """
    process = psutil.Process(os.getpid())
    mem_mb = process.memory_info().rss / 1024 / 1024
    logger.info(f"å½“å‰å†…å­˜å ç”¨: {mem_mb:.2f} MB")
    if force_restart or mem_mb > memory_limit_mb:
        logger.error(f"è§¦å‘è‡ªåŠ¨é‡å¯ï¼ˆforce={force_restart}ï¼Œå†…å­˜={mem_mb:.2f}MBï¼‰ï¼")
        try:
            # é‡å¯å‰å°†processingä»»åŠ¡é‡ç½®ä¸ºpending
            conn = sqlite3.connect(SQLITE_DB_PATH)
            cursor = conn.cursor()
            cursor.execute('UPDATE tasks SET status = "pending" WHERE status = "processing"')
            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"é‡å¯å‰é‡ç½®processingä»»åŠ¡å¤±è´¥: {e}")
        try:
            import torch
            torch.cuda.empty_cache()
        except Exception:
            pass
        os.execv(sys.executable, [sys.executable] + sys.argv)

def is_fatal_index_error(e):
    """
    åˆ¤æ–­æ˜¯å¦ä¸º index xxx is out of bounds for dimension 0 with size xxx ä¸”ä¸¤æ•°ç›¸ç­‰çš„è‡´å‘½é”™è¯¯
    æˆ– 'list index out of range' è¿™ç±»è‡´å‘½é”™è¯¯
    """
    msg = str(e)
    # æ£€æŸ¥ index out of bounds
    match = re.search(r'index (\d+) is out of bounds for dimension 0 with size (\d+)', msg)
    if match:
        idx, size = int(match.group(1)), int(match.group(2))
        if idx == size:
            return True
    # æ£€æŸ¥ list index out of range
    if "list index out of range" in msg:
        return True
    return False

def recognize_audio_with_timeout(asr_model, audio_file, timeout=60):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(recognize_audio, asr_model, audio_file)
        return future.result(timeout=timeout)
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=2000)
```



### Funasrå¤šèŠ‚ç‚¹å¤„ç†ä»»åŠ¡ä»£ç 

**å½“å‰å¦‚æœå¹¶å‘é‡å¤ªé«˜çš„è¿˜æ˜¯æœ‰ç‚¹é¡¶ä¸ä½ï¼Œæ‰€ä»¥éœ€è¦å†å¼€ä¸€ä¸ªèŠ‚ç‚¹å¤„ç†ä»»åŠ¡æ‰å¤„ç†å¾—è¿‡æ¥**

ä¸»ä»»åŠ¡

```python
import logging
import sys
from logging.handlers import TimedRotatingFileHandler
from datetime import datetime
import os
import psutil
import torch
import gc
import numpy as np
from scipy.io import wavfile
import re
import concurrent.futures

# æ—¥å¿—é…ç½®ï¼ŒåŠ¡å¿…æ”¾åœ¨æ‰€æœ‰importä¹‹å‰
log_formatter = logging.Formatter(
    "%(asctime)s %(levelname)s %(filename)s:%(lineno)d %(message)s"
)

# æ§åˆ¶å°æ—¥å¿—
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(log_formatter)

# æŒ‰å¤©åˆ‡åˆ†çš„æ–‡ä»¶æ—¥å¿—ï¼Œä¿ç•™7å¤©
file_handler = TimedRotatingFileHandler(
    "audio_recognition_api.log",
    when="midnight",           # æ¯å¤©0ç‚¹åˆ‡åˆ†
    interval=1,
    backupCount=7,             # ä¿ç•™æœ€è¿‘7å¤©æ—¥å¿—
    encoding="utf-8",
    utc=False
)
file_handler.setFormatter(log_formatter)

logging.basicConfig(
    level=logging.INFO,
    handlers=[console_handler, file_handler]
)

logger = logging.getLogger(__name__)

from fastapi import FastAPI, HTTPException, Request
from pydantic import BaseModel
import time
import os
import tempfile
from pathlib import Path
from funasr import AutoModel
from pydub import AudioSegment
import httpx
import asyncio
from concurrent.futures import ThreadPoolExecutor
import sqlite3
import threading
import requests
import pymysql
import uuid
import shutil

app = FastAPI()

TEMP_ROOT = Path("/root/autodl-tmp/FunASR/develop/wav_data")
# TEMP_ROOT = Path("D:/wav_data")
TEMP_ROOT.mkdir(parents=True, exist_ok=True)

# æ•°æ®åº“æ–‡ä»¶è·¯å¾„
SQLITE_DB_PATH = "tasks.db"

class RecognitionRequest(BaseModel):
    callRecordId: int
    fileUrl: str
    channelCode: str
    applyNo: str
    modelChannel: str
    callbackUrl: str
    doubaoEndpoint: str

def download_audio_from_url_sync(url, call_record_id, save_path=None):
    """åŒæ­¥ä¸‹è½½éŸ³é¢‘æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œæ–‡ä»¶åå”¯ä¸€"""
    try:
        response = requests.get(url, timeout=10, stream=True)
        response.raise_for_status()
        if save_path is None:
            temp_dir = TEMP_ROOT / f"funasr_temp_{call_record_id}_{uuid.uuid4().hex[:8]}"
            temp_dir.mkdir(exist_ok=True)
            save_path = temp_dir / f"downloaded_audio_{call_record_id}.wav"
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return str(save_path), temp_dir
    except Exception as e:
        return None, None

def split_stereo_audio(input_file, left_output_file, right_output_file, sample_rate=16000):
    audio = AudioSegment.from_wav(input_file)
    left = audio.split_to_mono()[0].set_frame_rate(sample_rate)
    right = audio.split_to_mono()[1].set_frame_rate(sample_rate)
    left.export(left_output_file, format="wav")
    right.export(right_output_file, format="wav")

# def ensure_wav_16k1ch(input_file, output_file):
#     """
#     è‡ªåŠ¨å°†éŸ³é¢‘è½¬æ¢ä¸º16kHzã€å•å£°é“ã€WAVæ ¼å¼
#     """
#     audio = AudioSegment.from_file(input_file)
#     audio = audio.set_frame_rate(16000).set_channels(1)
#     audio.export(output_file, format="wav")
#     return output_file

def recognize_audio(asr_model, audio_file):
    results = asr_model.generate(input=audio_file)
    return results

async def async_recognize_audio(asr_model, audio_file):
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor() as pool:
        result = await loop.run_in_executor(pool, recognize_audio, asr_model, audio_file)
    return result

# def check_audio_valid(audio_path, min_duration_ms=1000):
#     try:
#         audio = AudioSegment.from_wav(audio_path)
#         logger.info(f"éŸ³é¢‘æ£€æŸ¥: {audio_path}, æ—¶é•¿: {len(audio)}ms, é‡‡æ ·ç‡: {audio.frame_rate}, å£°é“: {audio.channels}")
#         if len(audio) < min_duration_ms:
#             raise Exception(f"éŸ³é¢‘è¿‡çŸ­: {len(audio)}ms")
#         if audio.frame_rate != 16000:
#             raise Exception(f"é‡‡æ ·ç‡ä¸æ˜¯16k: {audio.frame_rate}")
#         if audio.channels < 1:
#             raise Exception(f"å£°é“æ•°å¼‚å¸¸: {audio.channels}")
#         return True
#     except Exception as e:
#         logger.error(f"éŸ³é¢‘æœ‰æ•ˆæ€§æ£€æŸ¥å¤±è´¥: {audio_path}, error: {e}")
#         return False

def log_resource_usage():
    process = psutil.Process(os.getpid())
    logger.info(f"å†…å­˜å ç”¨: {process.memory_info().rss / 1024 / 1024:.2f} MB, æ‰“å¼€æ–‡ä»¶æ•°: {process.num_fds()}")

# åœ¨å…¨å±€åªåŠ è½½ä¸€æ¬¡æ¨¡å‹
asr_model = AutoModel(
    model="iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
    vad_model="fsmn-vad",
    punc_model="ct-punc",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
    spk_mode="punc_segment",
    sentence_timestamp=True,
    en_post_proc=True
)

def cleanup_resources():
    gc.collect()
    torch.cuda.empty_cache()

def resource_cleanup_loop(interval=600):
    while True:
        cleanup_resources()
        logger.info("å®šæœŸèµ„æºæ¸…ç†å®Œæˆ")
        time.sleep(interval)

cleanup_thread = threading.Thread(target=resource_cleanup_loop, daemon=True)
cleanup_thread.start()

@app.get("/health")
def health_check():
    mem = psutil.Process().memory_info().rss / 1024 / 1024
    return {"status": "ok", "memory_MB": mem, "threads": threading.active_count()}

@app.post("/recognize-audio")
async def recognize_audio_endpoint(request: RecognitionRequest):
    logger.info(f"æ”¶åˆ°æ–°ä»»åŠ¡è¯·æ±‚: callRecordId={request.callRecordId}, fileUrl={request.fileUrl}")
    try:
        conn = sqlite3.connect(SQLITE_DB_PATH)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                callRecordId INTEGER UNIQUE,
                fileUrl TEXT,
                channelCode TEXT,
                applyNo TEXT,
                modelChannel TEXT,
                callbackUrl TEXT,
                doubaoEndpoint TEXT,
                status TEXT
            )
        ''')
        try:
            cursor.execute('''
                INSERT INTO tasks (callRecordId, fileUrl, channelCode, applyNo, modelChannel, callbackUrl, doubaoEndpoint, status)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                request.callRecordId,
                request.fileUrl,
                request.channelCode,
                request.applyNo,
                request.modelChannel,
                request.callbackUrl,
                request.doubaoEndpoint,
                "pending"
            ))
            conn.commit()
            logger.info(f"ä»»åŠ¡å…¥åº“æˆåŠŸ: callRecordId={request.callRecordId}")
        except sqlite3.IntegrityError:
            logger.warning(f"ä»»åŠ¡å·²å­˜åœ¨: callRecordId={request.callRecordId}")
            conn.close()
            return {"message": f"ä»»åŠ¡å·²å­˜åœ¨ï¼ŒcallRecordId={request.callRecordId}"}
        conn.close()
        return {"message": "ä»»åŠ¡å·²å…¥åº“ï¼Œç¨åå›è°ƒç»“æœ"}
    except Exception as e:
        logger.error(f"ä»»åŠ¡å…¥åº“å¤±è´¥: callRecordId={request.callRecordId}, error={e}")
        raise HTTPException(status_code=500, detail=f"å…¥åº“å¤±è´¥: {e}")

@app.get("/process-pending-tasks")
def process_pending_tasks():
    logger.info("å¼€å§‹å¤„ç†pendingä»»åŠ¡")
    try:
        conn = sqlite3.connect(SQLITE_DB_PATH)
        cursor = conn.cursor()
        cursor.execute('SELECT id FROM tasks WHERE status = "pending"')
        task_ids = [row[0] for row in cursor.fetchall()]
        logger.info(f"æœ¬æ¬¡å¾…å¤„ç†ä»»åŠ¡æ•°: {len(task_ids)}")
        if not task_ids:
            conn.close()
            logger.info("æ— å¾…å¤„ç†ä»»åŠ¡")
            return {"message": "æ— å¾…å¤„ç†ä»»åŠ¡"}
        # å‡åŒ€åˆ†é…ä»»åŠ¡
        half = len(task_ids) // 2
        local_ids = task_ids[:half]
        worker_ids = task_ids[half:]
        processed = 0
        failed = 0
        # æœ¬åœ°å¤„ç†éƒ¨åˆ†
        if local_ids:
            cursor.executemany('UPDATE tasks SET status = "processing" WHERE id = ?', [(tid,) for tid in local_ids])
            conn.commit()
            cursor.execute(f'SELECT * FROM tasks WHERE id IN ({",".join(["?"]*len(local_ids))})', local_ids)
            tasks = cursor.fetchall()
            logger.info(f"æœ¬åœ°å¤„ç†ä»»åŠ¡æ•°: {len(tasks)}")
            for task in tasks:
                task_id, callRecordId, fileUrl, channelCode, applyNo, modelChannel, callbackUrl, doubaoEndpoint, status = task
                logger.info(f"å¼€å§‹æœ¬åœ°å¤„ç†ä»»åŠ¡: id={task_id}, callRecordId={callRecordId}")
                temp_dir = None
                try:
                    input_file, temp_dir = download_audio_from_url_sync(fileUrl, callRecordId)
                    if input_file is None:
                        raise Exception("æ— æ³•ä¸‹è½½éŸ³é¢‘æ–‡ä»¶")
                    standard_wav = input_file
                    left_audio = temp_dir / f"left_channel_{callRecordId}.wav"
                    right_audio = temp_dir / f"right_channel_{callRecordId}.wav"
                    split_stereo_audio(standard_wav, left_audio, right_audio)
                    try:
                        left_result = recognize_audio(asr_model, str(left_audio))
                    except Exception as e:
                        logger.error(f"å·¦å£°é“è¯†åˆ«å¤±è´¥: {e}")
                        left_result = f"VADè¯†åˆ«å¤±è´¥: {e}"
                        if is_fatal_index_error(e):
                            cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                            conn.commit()
                            failed += 1
                            cleanup_resources()
                            check_and_restart_if_oom(force_restart=True)
                            continue
                    try:
                        right_result = recognize_audio(asr_model, str(right_audio))
                    except Exception as e:
                        logger.error(f"å³å£°é“è¯†åˆ«å¤±è´¥: {e}")
                        right_result = f"VADè¯†åˆ«å¤±è´¥: {e}"
                        if is_fatal_index_error(e):
                            cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                            conn.commit()
                            failed += 1
                            cleanup_resources()
                            check_and_restart_if_oom(force_restart=True)
                            continue
                    processing_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    callback_data = {
                        "taskId": str(task_id),
                        "callRecordId": callRecordId,
                        "status": "success",
                        "processingTime": processing_time,
                        "leftChannelResult": str(left_result),
                        "rightChannelResult": str(right_result),
                        "errorMessage": "",
                        "channelCode": channelCode,
                        "applyNo": applyNo,
                        "modelChannel": modelChannel,
                        "doubaoEndpoint": doubaoEndpoint
                    }
                    response = requests.post(callbackUrl, json=callback_data)
                    response.raise_for_status()
                    cursor.execute('UPDATE tasks SET status = "completed" WHERE id = ?', (task_id,))
                    conn.commit()
                    processed += 1
                except Exception as e:
                    cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                    conn.commit()
                    logger.error(f"ä»»åŠ¡å¤„ç†å¤±è´¥: id={task_id}, callRecordId={callRecordId}, error={e}")
                    failed += 1
                finally:
                    if temp_dir and temp_dir.exists():
                        shutil.rmtree(temp_dir, ignore_errors=True)
                        logger.info(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
                    log_resource_usage()
                    cleanup_resources()
        # åˆ†é…ç»™workeréƒ¨åˆ†
        worker_result = None
        if worker_ids:
            try:
                # è¿™é‡Œå‡è®¾workeræœåŠ¡è¿è¡Œåœ¨127.0.0.1:2100ï¼Œä¸”æ”¯æŒPOST /process-pending-tasksï¼Œå‚æ•°ä¸ºjson: {"task_ids": [id1, id2, ...]}
                worker_url = "http://127.0.0.1:2100/process-pending-tasks"
                worker_result = requests.post(worker_url, json={"task_ids": worker_ids}, timeout=600).json()
                logger.info(f"workerå¤„ç†ç»“æœ: {worker_result}")
            except Exception as e:
                logger.error(f"workerè°ƒç”¨å¤±è´¥: {e}")
                # workerå¤±è´¥åå°†è¿™äº›ä»»åŠ¡é‡ç½®ä¸ºpending
                try:
                    cursor.executemany('UPDATE tasks SET status = \"pending\" WHERE id = ?', [(tid,) for tid in worker_ids])
                    conn.commit()
                    logger.info(f"workerå¤±è´¥ï¼Œå·²é‡ç½®{len(worker_ids)}ä¸ªä»»åŠ¡ä¸ºpending")
                except Exception as db_e:
                    logger.error(f"é‡ç½®workerä»»åŠ¡ä¸ºpendingå¤±è´¥: {db_e}")
        conn.close()
        return {"æœ¬åœ°å¤„ç†": f"æˆåŠŸï¼š{processed}ï¼Œå¤±è´¥ï¼š{failed}", "workerå¤„ç†": worker_result}
    except Exception as e:
        logger.error(f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")

def cleanup_resources():
    gc.collect()
    try:
        torch.cuda.empty_cache()
    except Exception:
        pass

def check_and_restart_if_oom(memory_limit_mb=16000, force_restart=False):
    """
    æ£€æŸ¥å†…å­˜å ç”¨ï¼Œæˆ–æ”¶åˆ°è‡´å‘½é”™è¯¯æ—¶å¼ºåˆ¶é‡å¯
    """
    process = psutil.Process(os.getpid())
    mem_mb = process.memory_info().rss / 1024 / 1024
    logger.info(f"å½“å‰å†…å­˜å ç”¨: {mem_mb:.2f} MB")
    if force_restart or mem_mb > memory_limit_mb:
        logger.error(f"è§¦å‘è‡ªåŠ¨é‡å¯ï¼ˆforce={force_restart}ï¼Œå†…å­˜={mem_mb:.2f}MBï¼‰ï¼")
        try:
            # é‡å¯å‰å°†processingä»»åŠ¡é‡ç½®ä¸ºpending
            conn = sqlite3.connect(SQLITE_DB_PATH)
            cursor = conn.cursor()
            cursor.execute('UPDATE tasks SET status = "pending" WHERE status = "processing"')
            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"é‡å¯å‰é‡ç½®processingä»»åŠ¡å¤±è´¥: {e}")
        try:
            import torch
            torch.cuda.empty_cache()
        except Exception:
            pass
        os.execv(sys.executable, [sys.executable] + sys.argv)

def is_fatal_index_error(e):
    """
    åˆ¤æ–­æ˜¯å¦ä¸º index xxx is out of bounds for dimension 0 with size xxx ä¸”ä¸¤æ•°ç›¸ç­‰çš„è‡´å‘½é”™è¯¯
    æˆ– 'list index out of range' è¿™ç±»è‡´å‘½é”™è¯¯
    """
    msg = str(e)
    # æ£€æŸ¥ index out of bounds
    match = re.search(r'index (\d+) is out of bounds for dimension 0 with size (\d+)', msg)
    if match:
        idx, size = int(match.group(1)), int(match.group(2))
        if idx == size:
            return True
    # æ£€æŸ¥ list index out of range
    if "list index out of range" in msg:
        return True
    return False

def recognize_audio_with_timeout(asr_model, audio_file, timeout=60):
    with concurrent.futures.ThreadPoolExecutor() as executor:
        future = executor.submit(recognize_audio, asr_model, audio_file)
        return future.result(timeout=timeout)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=2000)
```

workerä»»åŠ¡èŠ‚ç‚¹

```python
import logging
import sys
from logging.handlers import TimedRotatingFileHandler
from datetime import datetime
import os
import psutil
import torch
import gc
import numpy as np
from scipy.io import wavfile
import re
import concurrent.futures
from fastapi import FastAPI, HTTPException, Request, Body
from pydantic import BaseModel
import time
import tempfile
from pathlib import Path
from funasr import AutoModel
from pydub import AudioSegment
import httpx
import asyncio
from concurrent.futures import ThreadPoolExecutor
import sqlite3
import threading
import requests
import pymysql
import uuid
import shutil

# æ—¥å¿—é…ç½®
log_formatter = logging.Formatter(
    "%(asctime)s %(levelname)s %(filename)s:%(lineno)d %(message)s"
)
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(log_formatter)
file_handler = TimedRotatingFileHandler(
    "worker_funasr.log",
    when="midnight",
    interval=1,
    backupCount=7,
    encoding="utf-8",
    utc=False
)
file_handler.setFormatter(log_formatter)
logging.basicConfig(
    level=logging.INFO,
    handlers=[console_handler, file_handler]
)
logger = logging.getLogger(__name__)

app = FastAPI()

TEMP_ROOT = Path("/root/autodl-tmp/FunASR/develop/wav_data")
TEMP_ROOT.mkdir(parents=True, exist_ok=True)
SQLITE_DB_PATH = "tasks.db"

class RecognitionRequest(BaseModel):
    callRecordId: int
    fileUrl: str
    channelCode: str
    applyNo: str
    modelChannel: str
    callbackUrl: str
    doubaoEndpoint: str

def download_audio_from_url_sync(url, call_record_id, save_path=None):
    try:
        response = requests.get(url, timeout=10, stream=True)
        response.raise_for_status()
        if save_path is None:
            temp_dir = TEMP_ROOT / f"funasr_temp_{call_record_id}_{uuid.uuid4().hex[:8]}"
            temp_dir.mkdir(exist_ok=True)
            save_path = temp_dir / f"downloaded_audio_{call_record_id}.wav"
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        return str(save_path), temp_dir
    except Exception as e:
        return None, None

def split_stereo_audio(input_file, left_output_file, right_output_file, sample_rate=16000):
    audio = AudioSegment.from_wav(input_file)
    left = audio.split_to_mono()[0].set_frame_rate(sample_rate)
    right = audio.split_to_mono()[1].set_frame_rate(sample_rate)
    left.export(left_output_file, format="wav")
    right.export(right_output_file, format="wav")

def recognize_audio(asr_model, audio_file):
    results = asr_model.generate(input=audio_file)
    return results

def log_resource_usage():
    process = psutil.Process(os.getpid())
    logger.info(f"å†…å­˜å ç”¨: {process.memory_info().rss / 1024 / 1024:.2f} MB, æ‰“å¼€æ–‡ä»¶æ•°: {process.num_fds()}")

def cleanup_resources():
    gc.collect()
    try:
        torch.cuda.empty_cache()
    except Exception:
        pass

def is_fatal_index_error(e):
    msg = str(e)
    match = re.search(r'index (\d+) is out of bounds for dimension 0 with size (\d+)', msg)
    if match:
        idx, size = int(match.group(1)), int(match.group(2))
        if idx == size:
            return True
    if "list index out of range" in msg:
        return True
    return False

def check_and_restart_if_oom(memory_limit_mb=16000, force_restart=False):
    process = psutil.Process(os.getpid())
    mem_mb = process.memory_info().rss / 1024 / 1024
    logger.info(f"å½“å‰å†…å­˜å ç”¨: {mem_mb:.2f} MB")
    if force_restart or mem_mb > memory_limit_mb:
        logger.error(f"è§¦å‘è‡ªåŠ¨é‡å¯ï¼ˆforce={force_restart}ï¼Œå†…å­˜={mem_mb:.2f}MBï¼‰ï¼")
        try:
            conn = sqlite3.connect(SQLITE_DB_PATH)
            cursor = conn.cursor()
            cursor.execute('UPDATE tasks SET status = "pending" WHERE status = "processing"')
            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"é‡å¯å‰é‡ç½®processingä»»åŠ¡å¤±è´¥: {e}")
        try:
            import torch
            torch.cuda.empty_cache()
        except Exception:
            pass
        os.execv(sys.executable, [sys.executable] + sys.argv)

# å…¨å±€åªåŠ è½½ä¸€æ¬¡æ¨¡å‹
asr_model = AutoModel(
    model="iic/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-pytorch",
    vad_model="fsmn-vad",
    punc_model="ct-punc",
    vad_kwargs={"max_single_segment_time": 30000},
    device="cuda:0",
    spk_mode="punc_segment",
    sentence_timestamp=True,
    en_post_proc=True
)

@app.get("/health")
def health_check():
    mem = psutil.Process().memory_info().rss / 1024 / 1024
    return {"status": "ok", "memory_MB": mem, "threads": threading.active_count()}

@app.post("/process-pending-tasks")
def process_pending_tasks(request: Request, body: dict = Body(...)):
    logger.info("[worker] å¼€å§‹å¤„ç†pendingä»»åŠ¡ï¼ˆæŒ‡å®šidï¼‰")
    try:
        task_ids = body.get("task_ids", [])
        if not task_ids:
            logger.info("[worker] æœªæŒ‡å®šä»»åŠ¡idï¼Œç›´æ¥è¿”å›")
            return {"message": "æœªæŒ‡å®šä»»åŠ¡id"}
        conn = sqlite3.connect(SQLITE_DB_PATH)
        cursor = conn.cursor()
        cursor.executemany('UPDATE tasks SET status = "processing" WHERE id = ?', [(tid,) for tid in task_ids])
        conn.commit()
        cursor.execute(f'SELECT * FROM tasks WHERE id IN ({",".join(["?"]*len(task_ids))})', task_ids)
        tasks = cursor.fetchall()
        processed = 0
        failed = 0
        logger.info(f"[worker] æœ¬æ¬¡å¾…å¤„ç†ä»»åŠ¡æ•°: {len(tasks)}")
        for task in tasks:
            task_id, callRecordId, fileUrl, channelCode, applyNo, modelChannel, callbackUrl, doubaoEndpoint, status = task
            logger.info(f"[worker] å¼€å§‹å¤„ç†ä»»åŠ¡: id={task_id}, callRecordId={callRecordId}")
            temp_dir = None
            try:
                input_file, temp_dir = download_audio_from_url_sync(fileUrl, callRecordId)
                if input_file is None:
                    raise Exception("æ— æ³•ä¸‹è½½éŸ³é¢‘æ–‡ä»¶")
                standard_wav = input_file
                left_audio = temp_dir / f"left_channel_{callRecordId}.wav"
                right_audio = temp_dir / f"right_channel_{callRecordId}.wav"
                split_stereo_audio(standard_wav, left_audio, right_audio)
                try:
                    left_result = recognize_audio(asr_model, str(left_audio))
                except Exception as e:
                    logger.error(f"[worker] å·¦å£°é“è¯†åˆ«å¤±è´¥: {e}")
                    left_result = f"VADè¯†åˆ«å¤±è´¥: {e}"
                    if is_fatal_index_error(e):
                        cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                        conn.commit()
                        failed += 1
                        cleanup_resources()
                        check_and_restart_if_oom(force_restart=True)
                        continue
                try:
                    right_result = recognize_audio(asr_model, str(right_audio))
                except Exception as e:
                    logger.error(f"[worker] å³å£°é“è¯†åˆ«å¤±è´¥: {e}")
                    right_result = f"VADè¯†åˆ«å¤±è´¥: {e}"
                    if is_fatal_index_error(e):
                        cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                        conn.commit()
                        failed += 1
                        cleanup_resources()
                        check_and_restart_if_oom(force_restart=True)
                        continue
                processing_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                callback_data = {
                    "taskId": str(task_id),
                    "callRecordId": callRecordId,
                    "status": "success",
                    "processingTime": processing_time,
                    "leftChannelResult": str(left_result),
                    "rightChannelResult": str(right_result),
                    "errorMessage": "",
                    "channelCode": channelCode,
                    "applyNo": applyNo,
                    "modelChannel": modelChannel,
                    "doubaoEndpoint": doubaoEndpoint
                }
                response = requests.post(callbackUrl, json=callback_data)
                response.raise_for_status()
                cursor.execute('UPDATE tasks SET status = "completed" WHERE id = ?', (task_id,))
                conn.commit()
                processed += 1
            except Exception as e:
                cursor.execute('UPDATE tasks SET status = "failed" WHERE id = ?', (task_id,))
                conn.commit()
                logger.error(f"[worker] ä»»åŠ¡å¤„ç†å¤±è´¥: id={task_id}, callRecordId={callRecordId}, error={e}")
                failed += 1
            finally:
                if temp_dir and temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    logger.info(f"[worker] å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
                log_resource_usage()
                cleanup_resources()
        conn.close()
        logger.info(f"[worker] æœ¬æ¬¡å¤„ç†å®Œæˆï¼ŒæˆåŠŸï¼š{processed}ï¼Œå¤±è´¥ï¼š{failed}")
        return {"message": f"å¤„ç†å®Œæˆï¼ŒæˆåŠŸï¼š{processed}ï¼Œå¤±è´¥ï¼š{failed}"}
    except Exception as e:
        logger.error(f"[worker] å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=2100) 
```



















